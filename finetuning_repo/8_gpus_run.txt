deepspeed --num_gpus=8 run_clm.py --deepspeed ds_config_stage1.json --model_name_or_path EleutherAI/gpt-j-6B --train_file train.csv --validation_file validation.csv --do_train --do_eval --bf16 --overwrite_cache --evaluation_strategy=steps --output_dir finetuned --num_train_epochs 100  --eval_steps 750 --gradient_accumulation_steps 1 --per_device_train_batch_size 4 --per_device_eval_batch_size 64 --use_fast_tokenizer False --learning_rate 5e-06 --warmup_steps 1500 --save_total_limit 1 --save_steps 750 --save_strategy steps --tokenizer_name gpt2 --load_best_model_at_end=True --block_size=128 --weight_decay=0.1 --report_to=wandb --group_texts True